---
title: "Assignment 4 - Applying meta-analytic priors"
author: "Riccardo Fusaroli"
date: "3/16/2018"
output: html_document
---

```{r}

setwd("C:/Users/emily/Desktop/Uni/Rstudio/R - Datascripts/Comp. Modelling/Portfolio-4")
library(readxl)
mData <- read_excel("Assignment4MetaData.xlsx")
#install.packages("metafor")
library(lme4);library(brms);library(tidyverse);library(ggplot2);library(rethinking);library(rstan)
```

## Assignment 4

In this assignment we do the following:
- we reproduce the meta-analysis of pitch SD from last semester in a Bayesian framework
- we reproduce the pitch SD in schizophrenia analysis from last semester using both a conservative and a meta-analytic prior
- we assess the difference in model quality and estimates using the two priors.

The questions you need to answer are: What are the consequences of using a meta-analytic prior? Evaluate the models with conservative and meta-analytic priors. Discuss the effects on estimates. Discuss the effects on model quality. Discuss the role that meta-analytic priors should have in scientific practice. Should we systematically use them? Do they have drawbacks? Should we use them to complement more conservative approaches? How does the use of meta-analytic priors you suggest reflect the skeptical and cumulative nature of science?

### Step by step suggestions

Step 1: Reproduce the meta-analysis of pitch sd from previous studies of voice in schizophrenia
- the data is available as Assignment4MetaData.xlsx
- Effect size (cohen's d), sd and variance are already calculated (you're welcome!)
- Since we're only interested in getting a meta-analytic effect size, let's take a shortcut and use bromance magic (brms): https://mvuorre.github.io/post/2016/2016-09-29-bayesian-meta-analysis/

```{r}

str(mData)
library(ggplot2)
ggplot(mData, aes(x=MeanES, y=StudyRef)) +
    geom_segment(aes(x = MeanES-SdES*2, xend = MeanES+SdES*2, y=StudyRef, yend=StudyRef)) +
    geom_point()

#Analyse difference in Pitch between TD and SD children. The studies could measure the pitch in different measures, scale all data into same scale - difference in SD. 
#Mean of estimate (Cohens d) Mean 1 - mean 2 / SD = cohen d. Estimated effect size
#SdEs, sd of estimate. Sd of cohens d. Include uncertainty. Uncertainty. The more, the less weight should be given to it. The more uncertain the less impact it should have on the overall mean.

#Both gives metanalytical prior.

#brm(glmer) formula of model

#outcome (mean effect size)| SE(sdES) (add the uncertainty, SD) ~ 1(is an effect of an intercept. Gives us an effectsize across all studies. Maximize the likelihood. Intercept give us a mean) + (random effect 1|StudyRef), data, + 4 more variables(cores, chain, iter), prior (add prior, for bromance it automatically calculate it, so we don't need to add it)  
# devtools::install_github("stan-dev/rstan", ref = "develop", subdir = "rstan/rstan", build_vignettes = FALSE, dependencies = TRUE)

M <- brm(MeanES|se(SdES)~ 1 + (1|StudyRef), data = mData, cores = 2, chain= 2, iter=2000)
summary(M)

#3library(brmstools)
#devtools::install_github("mvuorre/brmstools")
#Forrest plot
forest(M)

```


Step 2: Prepare the pitch SD data from last year
- the data is available as Assignment4PitchData.csv (thanks Celine)
- We do not know how to build random effects, yet. So we need to simplify the dataset to avoid interdependence between datapoint: How?
- Also, let's standardize the data, so that they are compatible with our meta-analytic prior (Cohen's d is measured in SDs). 
#simplify data, make 1 datapoint pr- participant.
#scale data so it is compatible with cohens d

```{r}
#Find mean pr. trial 
PD <- read_excel("Assignment4PitchDatav2.xlsx")

library(dplyr)
str(PD)
df1 <- subset(PD) %>%
  group_by(ID_unique) %>%
  # dplyr::summarise_all(funs(mean))%>%
   dplyr::summarise(diagnosis = mean(diagnosis), PitchMean = mean(PitchMean), PitchSD = mean(PitchSD), PitchMedian = mean(PitchMedian), PitchRange = mean(PitchRange), PitchIQR = mean(PitchIQR),PitchMad = mean(PitchMad), PitchCV = mean(PitchCV)) %>%
  mutate(sPitchMean = scale(PitchMean),
sPitchSD = scale(PitchSD),
sPitchMedian = scale(PitchMedian),
sPitchRange = scale(PitchRange),
sPitchIQR = scale(PitchIQR),
sPitchMad = scale(PitchMad),
sPitchCV = scale(PitchCV))

```


Step 3: Build a regression model predicting Pitch SD from Diagnosis.
- how is the outcome distributed? (likelihood function)
- how are the parameters of the likelihood distribution distributed? Which predictors should they be conditioned on?
- use a skeptical/conservative prior for the effects of diagnosis. Remember you'll need to motivate it.
- Describe and plot the estimates. Evaluate model quality
```{r}
#Build model. Asses which difference it makes using a skeptical and metanalytical prior.
#Outcome Pitch SD, how is it distributed, we just pick normal distribution. We are doing a specific operation, sampling across participants getting means. When we keep sampling one from on distribution and then get a mean, eventually they will be gaussian. 

#define mu. What is oir expected meanpitch SD. mu <- alpha + beta(mean difference between control and shitzo)*diagnosis(mean value for diagnosis).
#Prior for alpha, we want a mean of 0 and SD of 1. Normal (0,0.5)
#Prior for beta, what distribution? Mean of 0 and SD of 1.
#Sigma, which is the expected error of our prediction. 
#Can do a decauchy sigma, sigma ~ decauchy(0,3)
#Doing a log of sigma we tell it to always make a positive number.
#Sigma, log(sigma) = alphaSigma + BetaSigma*diagnosis

#In STAN
#mu -> alpha[participant(ID)] + b[participant]*diagnosis

df1 <- as.data.frame(df1)

M2 <- map2stan(
  alist(
    sPitchSD ~ dnorm(mu, sigma),
    mu <- a + b*diagnosis,
    a ~ dnorm(0,0.5),
    b ~ dnorm(0,0.2),
    sigma ~ dcauchy(0,3)
  ), data = df1, chains = 4, cores = 2, iter = 5000, warmup = 2000
)

precis(M2)

#PLOTSPLOTSPLOTSPLOTSPLOTSTSTSTS for funs .. p. 251 book
plot(M2)
pairs(M2)

#Make density plots from sim values
#Dens quality plot
sim.PD2 <- sim(M2, data = df1)

dens(sim.PD2, col = "red", xlim = c(-3, 2.5), ylim = c(0, 1),  xlab = "PitchSD")
par(new=TRUE)
dens(df1$sPitchSD, xlim = c(-3,2.5), ylim = c(0,1), xlab = "PitchSD")


```


Step 4: Now re-run the model with the meta-analytic prior
- Describe and plot the estimates. Evaluate model quality

#Estimates
Intercept: -0.54,
SE(INT): 0.23
SD(Int): 0.71
SE(SD(INT)): 0.22
```{r}

M3 <- map2stan(
  alist(
    sPitchSD ~ dnorm(mu, sigma),
    mu <- a + b*diagnosis,
    a ~ dnorm(0,0.5),
    b ~ dnorm(-0.54,0.23),
    sigma ~ dcauchy(0,3)
  ), data = df1, chains = 4, cores = 2, iter = 5000, warmup = 2000
)

summary(M3)
#PLOTSPLOTSPLOTSPLOTSPLOTSTSTSTS .. p. 251 book
fun1 <- plot(M3)
fun2 <- pairs(M3)

#Dens quality plot
sim.PD3 <- sim(M3, data = df1)

dens(sim.PD3, col = "red", xlim = c(-3, 2.5), ylim = c(0, 1),  xlab = "PitchSD")
par(new=TRUE)
dens(df1$sPitchSD, xlim = c(-3,2.5), ylim = c(0,1), xlab = "PitchSD")

```

Step 5: Compare the models
- Plot priors and posteriors of the diagnosis effect in both models
- Compare posteriors between the two models
- Compare their relative distance from truth (WAIC)
```{r}

#Compare
postP1 <- plot(coeftab(M2,M3))

compare(M2, M3)


#plot - Prior/posterior, Model 2
samples = extract.samples(M2, n =1e4)
samples$bprior = rnorm(1e4, 0, 0.2)

type= rep(c("posterior","prior"), each =1e4)
value =c(t(samples$b), t(samples$bprior))
d = data.frame(value, type)

priorP1 <- ggplot(d,aes(value,group =type, color = type)) +geom_density()+ggtitle("Sceptical Model")

#plot, prior/posterior - Model 3

samples2 = extract.samples(M3, n =1e4)
samples2$bprior = rnorm(1e4,-0.54, 0.23)

type= rep(c("posterior","prior"), each =1e4)
value =c(t(samples2$b), t(samples2$bprior))
d1 = data.frame(value, type)

priorP1.a <- ggplot(d1,aes(value,group =type, color = type)) +geom_density() + ggtitle("Meta Model")



#Kenneth prior plot.
# x <- seq(-2,2, length=1e5)
# y.s <- dnorm(x, 0, 0.2) #sceptical
# y.m <- dnorm(x, -0.54, 0.23) #meta
# prior_df <- data.frame(x = rep(x,2), y = c(y.s, y.m), prior = c(                                                                  rep("sceptical", length(y.s)),
#                                                                    rep("meta", length(y.m))
#                                                                    ))
# priorP2 <- ggplot(prior_df, aes(x = x, y = y, color = prior)) + geom_line()




```
- Discuss how they compare and whether any of them is best.
#We can see that the two models are very similar, WAIC M3 = 432.5, M2= 432.6, M3 is "better", but both have large SE, so there is no real difference. The sceptical M2 overlaps with zero, suggesting maybe there is a slight bias in the litterature. 

Step 6: Prepare a nice write up of the analysis and answer the questions at the top.

Optional step 7: how skeptical should a prior be?
- Try different levels of skepticism and compare them using WAIC.

Optional step 8: Include other predictors
- Do age, gender and education improve the model?
- Should they be main effects or interactions?

Optional step 9: Bromance magic.
- explore the bromance code below including random effects (by default with weakly informative priors)
- learn how to change the prior
- explore effects of trial, age, gender, including the appropriate random slopes
- compare the models you created using WAIC and posterior predictive check (pp_check())


```{r}

brm_out <- brm(PitchSD ~ 1 + Diagnosis  +(1|ID_unique/Study), # Outcome as a function of the predictors as in lme4. 
               data=Data, # Define the data
               family=gaussian(), # Define the family. 
               iter = 5000, warmup = 2000, cores = 4)
summary(brm_out1)
plot(brm_out1)

```

